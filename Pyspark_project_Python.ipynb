{"cells":[{"cell_type":"code","source":["# The dataset is from Kaggle lending club loan dataset with below link:\n# https://www.kaggle.com/wendykan/lending-club- loan-data\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import StandardScaler\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nimport pandas as pd\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["from pyspark.sql import SQLContext\nsqlc = SQLContext(sc)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["df = (sqlc.read.format('com.databricks.spark.csv')\n      .options(header='true', inferschema='true')\n      .load('/FileStore/tables/d20jc3dn1501724096519/loan.csv'))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["df.count()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["df.groupBy(\"loan_status\").count().show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql.types import StringType\nfrom pyspark.sql.functions import udf\ndef modify_values(r):\n    if r == \"Default\" or r ==\"Charged Off\" or r ==\"Late (31-120 days)\" or r ==\"Late (16-30 days)\" or r ==\"Does not meet the credit policy. Status:Charged Off\" :\n        return int(1)\n    else:\n        return int(0)\n\n\nol_val = udf(modify_values, StringType())\nnew_df = df.withColumn(\"default_Binary\",ol_val(df.loan_status))\n  "],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["new_df.groupby(\"default_Binary\").count().show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["df_1 = new_df.select( \"Default_Binary\",\n                     \"loan_amnt\",\n                     \"int_rate\",\n                     \"grade\",\n                     \"emp_length\",\n                     \"purpose\"\n                    )"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["pd.DataFrame(df_1.take(5), columns=df_1.columns).transpose()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["df_1.describe().show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#fill numeria variavle na with mean\nnumVars = ['loan_amnt','int_rate']\ndef countNull(df_1,var):\n    return df_1.where(df_1[var].isNull()).count()\nmissing = {var: countNull(df_1,var) for var in numVars}\nload_amnt_mean = df_1.groupBy().mean('loan_amnt').first()[0]\nint_rate_mean = df_1.groupBy().mean('int_rate').first()[0]\ndf_1 = df_1.na.fill({'loan_amnt':load_amnt_mean,'int_rate':int_rate_mean})"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["df_1=df_1.dropna()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["catVars = ['grade','emp_length','purpose']\n## make use of pipeline to index all categorical variables\ndef indexer(df_1,col):\n    si = StringIndexer(inputCol = col, outputCol = col+'_indexed').fit(df_1)\n    return si\nindexers = [indexer(df_1,col) for col in catVars]\nfrom pyspark.ml import Pipeline\npipeline_1 = Pipeline(stages = indexers)\ndf_2 = pipeline_1.fit(df_1).transform(df_1)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["df_2.printSchema()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["df_2.select(\"grade\",\"grade_indexed\").distinct().show()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#covert features to Vectors\ncatVarsIndexed = [i+'_indexed' for i in catVars]\nfeaturesCol = numVars+catVarsIndexed\nlabelCol = ['Default_Binary']\nfrom pyspark.sql import Row\nrow = Row('label','features') \ndf_2 = df_2[labelCol+featuresCol]"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["df_2.show(5)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["df_2.groupby('Default_Binary').count().toPandas()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# map features to DenseVector\nfrom pyspark.ml.linalg import DenseVector\nlf = df_2.rdd.map(lambda r: (row(r[0],DenseVector(r[1:])))).toDF()\nlf = StringIndexer(inputCol = 'label',outputCol='index').fit(lf).transform(lf)\nlf.show(5)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["(trainingData, testData) = lf.randomSplit([0.7, 0.3], seed = 100)\nprint trainingData.count()\nprint testData.count()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# logistic regression\nfrom pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(maxIter = 10, regParam = 0.05, labelCol='index')\nlrmodel=lr.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# Evaluate model based on auc ROC(default for binary classification)\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n \ndef testModel(model, testData = testData):\n    pred = model.transform(testData)\n    evaluator = BinaryClassificationEvaluator(labelCol = 'index')\n   \n    return evaluator.evaluate(pred),pred.select('index', 'prediction', 'probability').toPandas().sample(n=5)\nprint 'AUC ROC of Logistic Regression model is: '+str(testModel(lrmodel))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["#decision tree random forest\nfrom pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n \ndt = DecisionTreeClassifier(maxDepth = 4,  impurity='gini', labelCol ='index')\ndtmodel=dt.fit(trainingData)\nrf = RandomForestClassifier(numTrees = 100, labelCol = 'index')\nrfmodel=rf.fit(trainingData)\n \nmodels = {'LogisticRegression':lrmodel,\n          'DecistionTree':dtmodel,\n          'RandomForest':rfmodel}\n \nmodelPerf = {k:testModel(v) for k,v in models.iteritems()}\n \nprint modelPerf"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["#Stratified Sampling\nstratified_data = lf.sampleBy(\"index\", fractions={0: 0.1, 1.0: 1.0}).cache()\n\nstratified_data.groupby(\"index\").count().toPandas()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["(trainingData2, testData2) = stratified_data.randomSplit([0.7, 0.3], seed = 100)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["lrModel2 = lr.fit(trainingData2)\npredictions = lrModel2.transform(testData2)\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(labelCol = 'index')\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(labelCol = 'index')\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())\ncv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData2)\n# this will likely take a fair amount of time because of the amount of models that we're creating and testing\npredictions = cvModel.transform(testData2)\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["dtModel2 = dt.fit(trainingData2)\npredictions = dtModel2.transform(testData2)\nprint(dtModel2.toDebugString)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# Evaluate model\nevaluator = BinaryClassificationEvaluator(labelCol = 'index')\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["paramGrid = (ParamGridBuilder()\n             .addGrid(dt.maxDepth, [1,2,6,10])\n             .addGrid(dt.maxBins, [20,40,80])\n             .build())\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData2)\n\n# Use test set here so we can measure the accuracy of our model on new data\npredictions = cvModel.transform(testData2)\n# Evaluate best model\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# getting random forest feature importances\nrfModel2 = rf.fit(trainingData2)\npredictions = rfModel2.transform(testData2)\nprint (rfModel2.featureImportances)\n"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# Evaluate model\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nevaluator = BinaryClassificationEvaluator(labelCol = 'index')\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(rf.maxDepth, [2, 4, 6])\n             .addGrid(rf.maxBins, [20, 60])\n             .addGrid(rf.numTrees, [5, 20])\n             .build())\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\ncvModel = cv.fit(trainingData2)\npredictions = cvModel.transform(testData2)\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["bestModel = cvModel.bestModel\nfinalPredictions = bestModel.transform(stratified_data)\nevaluator.evaluate(finalPredictions)"],"metadata":{},"outputs":[],"execution_count":36}],"metadata":{"name":"Term_project_Python","notebookId":3404770352268224},"nbformat":4,"nbformat_minor":0}
